{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Evaluation and Selection</h1>\n",
    "\n",
    "<p>In this assignment we will introduce some techniques to evaluate the quality of a method and how to select good parameter values.</p>\n",
    "\n",
    "<p>We will be using the scikit built-in breast_cancer data set. It is binary classification problem where breast masses are classified as malignin (equal 0) or benign (equal 1).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "data = dataset.data\n",
    "target = dataset.target\n",
    "\n",
    "###Find how many features we have and their names\n",
    "\n",
    "\n",
    "#The columns 10 to 19 are measurements errors and we can drop them without affecting much the work done here\n",
    "###Remove the columns 10 to 19 in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example on a Single Decision Tree</h2>\n",
    "\n",
    "<p>In this section we will introduce evaluation and paramtere selection techniques on a single decision tree.</p>\n",
    "\n",
    "<h4>Simple Evaluation</h4>\n",
    "<p>Evaluating the accuracy of a method can naively be done by splitting the data set in a training set and a test set.\n",
    "We train our classifier on the training set (obviously) and we evaluate the accuracy on the test set.<br>\n",
    "In scikit this is easily done by using the <i>.score()</i> functions of the classifier.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "###Split the data in train and test sets and the target in train_target and test_target (ratio 70%-30%)\n",
    "## Hint : by using the keyword \"random_state=0\" when you call train_test_split\n",
    "##        you make sure that the splits are the same for both data and target\n",
    "\n",
    "\n",
    "###Import a decision tree and train it on the training set with the default settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Compute the accuracy on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The accuracy is simply giving you the amount of samples that have been correctly classified<br>\n",
    "Other methods to measure the quality of the classifier are available. For instance one can use the F1 score. F1 score use the <i>precision</i> and <i>recall</i> (see https://en.wikipedia.org/wiki/Precision_and_recall) to evaluate the quality of a classification.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#f1_score(<test targets>,<classifier prediction for the test set>,average='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It is also possible to have the detail of precision and recall for both classes :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#print classification_report(<test targets>,<classifier predictions for test set>,target_names=<target names>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We started this study by doing a random separation for the train/test sets. Actually all scores of tests performed so far depend on this separation.</p>\n",
    "\n",
    "<h4> <u>QUESTION 1 :</u> Explain why all scores are specific to our first sets split.</h4>\n",
    "<p><i>Type your answer here</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross Validation</h4>\n",
    "\n",
    "<p>Another way to evaluate the accuracy of a decision tree (and any method in general) is to use a cross validation technique :\n",
    "Basically it consists in first dividing the data set in <i>k</i> sets named <i>folds</i>, then train the classifier on <i>k-1</i> folds and evaluate the accuracy on the remaining fold.</p>\n",
    "<p>In scikit this can be done by using the <i>cross_val_score</i> function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "###use the cross_val_score function on the whole dataset\n",
    "\n",
    "\n",
    "#When calling the cross_val_score it returns one score per fold\n",
    "#As by default the function uses a three-fold separation you have three value\n",
    "###Compute and print the mean and the standard deviation of the cross_val_score function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Several techniques exist to divide the data set in folds (see http://scikit-learn.org/stable/modules/cross_validation.html for more details).</p>\n",
    "<p>Nonetheless, it is worth mentioning another technique : the ShuffleSplit. This technique generates a pre-defined number of independent train/test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.</p>\n",
    "<p>This can be implemented as follow :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "cv_ss = ShuffleSplit(data.shape[0],n_iter=5,test_size=0.4,random_state=0)\n",
    "\n",
    "###Use again the cross_val_score function and set \"cv=cv_ss\"\n",
    "\n",
    "###Compute again the mean and the standard deviation of the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 2 :</u> Are results very different?</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<h3>Finding Optimal Parameters</h3>\n",
    "\n",
    "<p>In the previous section we have use the default settings for our classifiers but this is usually not necessarily the most optimal choice.</p>\n",
    "<p>In this section we will introduce tools to find good parameters value.</p>\n",
    "\n",
    "<h4>Grid Search - a brute force approach</h4>\n",
    "<p>A decision tree has several parameters we can change to optimize the classification. Scikit offers the possibility to investigate several parameters using <i>GridSearchCV</i>. You simply need to define a \"parameter grid\" (a dictionary in python) that defines the parameters values you want to try and feed it to a GridSearchCV object :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-22f9844c72c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#grd = GridSearchCV(<classifier>,cv=3,param_grid=<dictionary of parameters to investigate>)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#grd.fit(<training set>,<train set targets>)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgrd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mgrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtr' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "###Example of a parameter grid dictionary, run it once and then include more parameters in p_grid\n",
    "p_grid=dict({'criterion':['entropy','gini']})\n",
    "\n",
    "#grd = GridSearchCV(<classifier>,cv=3,param_grid=<dictionary of parameters to investigate>)\n",
    "#grd.fit(<training set>,<train set targets>)\n",
    "grd = GridSearchCV(dtr,cv=3,param_grid=param_grid)\n",
    "grd.fit(train,train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 2 :</u> What does the \"CV\" at the end of GridSearchCV stands for? What is it telling you?</h4>\n",
    "<p><i>Type your answer here</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d337d10aeb16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#You can ask for the best parameters found by running the following command\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'grd' is not defined"
     ]
    }
   ],
   "source": [
    "#You can ask for the best parameters found by running the following command\n",
    "grd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4f3d27d2cdfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#And creat directly a new classifier with the optimal parameters by running\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_dtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m###Check the \"new_dtr\" parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grd' is not defined"
     ]
    }
   ],
   "source": [
    "#And creat directly a new classifier with the optimal parameters by running\n",
    "new_dtr = grd.best_estimator_\n",
    "\n",
    "###Check the \"new_dtr\" parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Now train the new classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Compute its accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Learning and Validation Curves</h4>\n",
    "<p> Scikit provides additional tools to tune our algorithm.<br>\n",
    "One useful tool is the learning curve. It gives the cross-validated training and test scores for different training sets sizes.\n",
    "We can use it on the previously defined new classifier \"new_dtr\" :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "###Compute the learning curve\n",
    "#<number of elements in train>,<train score>,<test score> = learning_curve(<classifier>,<data>,<targets>,train_sizes=<liste of training sizes>,cv=3)\n",
    "#The list of training sizes can be absolute numbers or amount if between (0,1]\n",
    "\n",
    "###Visualize the learning curve (don't forget labels, title,legend,etc)\n",
    "\n",
    "#You should see why I chose a 70%-30% ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 3 :</u> Why isn't the training score equal to one?</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<p>A second tool meant to investigate a specific parameter influence on scores is the <i>validation curve</i>. It is basically like a gridsearch with a single parameter. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_dtr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6d10d9106e2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Just an example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_dtr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m###Plot the validation curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_dtr' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "#Just an example\n",
    "train_score,test_score = validation_curve(new_dtr,data,target,param_name='max_depth',param_range=np.arange(2,8),cv=3)\n",
    "\n",
    "###Plot the validation curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Application - Evaluating the Random Forest classifier and the SVC</h3>\n",
    "\n",
    "<p>In the following you will apply the evaluation and optimization tools to compare the Random Forest technique and the SVC technique.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Import the \"RandomForestClassifier\" classifier\n",
    "\n",
    "###Train it on the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Import the \"SVC\" classifier\n",
    "\n",
    "###Train it on the training set  (don't forget to scale it!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before evaluating the performance of both classifiers we will first determine the best values for their parameters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Using the grid search optimize the Random Forest Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Create a new Random Forest Classifier using the best parameters found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Using the grid search optimize the SVC Classifier\n",
    "#(!don't forget to scale the data!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Create a new Random Forest Classifier using the best parameters found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we have optimized out two classifier we can compare how they perform</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Compute and the accuracy of both classifiers on the training set using the cross_val_score\n",
    "# (!scale for SVC!)\n",
    "\n",
    "\n",
    "#Print the results (average and std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 4 :</u> Which classifier gives the best accuracy?</h4>\n",
    "<p><i>Type your answer here</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Print the Classification report for the Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Do the same for the SVC classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 5 :</u> Analyze the last two classification reports.</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<h4> <u>QUESTION 6 :</u> Recall the classification report from the optimized decision tree to conclude on the best algorithm to chose to efficiently detect malignant masses.</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<h4> <u>BONUS :</u> Repeat the optimization and evalution procedure with the k-nearest neighbors approach.</h4>\n",
    "<p><i>Type your answer here</i></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
